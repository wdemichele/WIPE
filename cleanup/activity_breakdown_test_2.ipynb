{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import json\n",
                "import logging\n",
                "from typing import List, Tuple, Dict\n",
                "from collections import defaultdict\n",
                "import math\n",
                "from dataclasses import asdict\n",
                "from src.common.prompts.value_adding_analysis.gpt_components import PromptComponents\n",
                "\n",
                "ROOT_DIR = r\"C:\\Projects\\Research\\SWEEP\\SWEEP\"\n",
                "\n",
                "# Set up logging\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "\n",
                "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
                "sys.path.append(root_path)\n",
                "\n",
                "from src.common.models.process_model import ProcessModel\n",
                "from src.value_adding_analysis import ValueClassificationComponentsGPT\n",
                "from src.value_adding_analysis.value_adding_analysis_metrics import compare_value_classifications, print_comparison_results\n",
                "\n",
                "def get_sectors(train=True) -> List[str]:\n",
                "    return [sector for sector in os.listdir(\"data\") if os.path.isdir(os.path.join(\"data\",f\"{'train' if train else 'test'}\", sector))]\n",
                "\n",
                "def get_activities(sector: str, train=True) -> List[str]:\n",
                "    sector_path = os.path.join(\"data\",f\"{'train' if train else 'test'}\", sector)\n",
                "    return [activity for activity in os.listdir(sector_path) if os.path.isdir(os.path.join(sector_path, activity))]\n",
                "\n",
                "def get_file_paths(sector: str, activity: str, model_name: str, train=True) -> Tuple[str, str, str, str]:\n",
                "    dir_path = os.path.join(ROOT_DIR, \"data\", f\"{'train' if train else 'test'}\", sector, activity)\n",
                "    test_path = os.path.join(ROOT_DIR, \"test\", \"results\", sector, activity)\n",
                "    activity_breakdown_path = os.path.join(ROOT_DIR, dir_path, f\"{activity}_activity_breakdown.json\")\n",
                "    ground_truth_path = os.path.join(ROOT_DIR, dir_path, f\"{activity}_step_value_analysis.json\")\n",
                "    response_path = os.path.join(ROOT_DIR, test_path, f\"{model_name}_response.json\")\n",
                "    return test_path, activity_breakdown_path, ground_truth_path, response_path\n",
                "\n",
                "def create_prompt_components_variations() -> List[Dict[str, PromptComponents]]:\n",
                "    variations = [\n",
                "        {\n",
                "            \"name\": \"Neutral_Analyst_Basic\",\n",
                "            \"components\": PromptComponents.from_dict({\n",
                "                \"role_description\": \"neutral_analyst\",\n",
                "                \"task_description\": \"standard\",\n",
                "                \"classification_types\": \"basic\",\n",
                "                \"function_definition\": \"basic\",\n",
                "                \"parsing_instructions\": \"sequential\",\n",
                "                \"output_format\": \"basic\"\n",
                "            })\n",
                "        },\n",
                "        {\n",
                "            \"name\": \"Lean_Expert_Detailed\",\n",
                "            \"components\": PromptComponents.from_dict({\n",
                "                \"role_description\": \"lean_expert\",\n",
                "                \"task_description\": \"efficiency_focused\",\n",
                "                \"classification_types\": \"detailed\",\n",
                "                \"function_definition\": \"detailed\",\n",
                "                \"parsing_instructions\": \"holistic\",\n",
                "                \"output_format\": \"structured\",\n",
                "                \"additional_guidelines\": \"lean_principles\"\n",
                "            })\n",
                "        },\n",
                "        {\n",
                "            \"name\": \"Process_Engineer_Technical\",\n",
                "            \"components\": PromptComponents.from_dict({\n",
                "                \"role_description\": \"process_engineer\",\n",
                "                \"task_description\": \"standard\",\n",
                "                \"classification_types\": \"detailed\",\n",
                "                \"function_definition\": \"basic\",\n",
                "                \"parsing_instructions\": \"sequential\",\n",
                "                \"output_format\": \"basic\",\n",
                "                \"example_output\": \"complex_process\"\n",
                "            })\n",
                "        },\n",
                "        {\n",
                "            \"name\": \"Customer_Advocate_ValueFocused\",\n",
                "            \"components\": PromptComponents.from_dict({\n",
                "                \"role_description\": \"customer_advocate\",\n",
                "                \"task_description\": \"efficiency_focused\",\n",
                "                \"classification_types\": \"basic\",\n",
                "                \"function_definition\": \"detailed\",\n",
                "                \"parsing_instructions\": \"holistic\",\n",
                "                \"output_format\": \"structured\",\n",
                "                \"additional_guidelines\": \"context_aware\"\n",
                "            })\n",
                "        },\n",
                "        {\n",
                "            \"name\": \"Business_Consultant_Strategic\",\n",
                "            \"components\": PromptComponents.from_dict({\n",
                "                \"role_description\": \"business_consultant\",\n",
                "                \"task_description\": \"standard\",\n",
                "                \"classification_types\": \"detailed\",\n",
                "                \"function_definition\": \"detailed\",\n",
                "                \"parsing_instructions\": \"holistic\",\n",
                "                \"output_format\": \"structured\",\n",
                "                \"example_output\": \"complex_process\",\n",
                "                \"additional_guidelines\": \"context_aware\"\n",
                "            })\n",
                "        },\n",
                "        {\n",
                "            \"name\": \"Neutral_Analyst_Comprehensive\",\n",
                "            \"components\": PromptComponents.from_dict({\n",
                "                \"role_description\": \"neutral_analyst\",\n",
                "                \"task_description\": \"efficiency_focused\",\n",
                "                \"classification_types\": \"detailed\",\n",
                "                \"function_definition\": \"detailed\",\n",
                "                \"parsing_instructions\": \"holistic\",\n",
                "                \"output_format\": \"structured\",\n",
                "                \"example_output\": \"complex_process\",\n",
                "                \"additional_guidelines\": \"lean_principles\"\n",
                "            })\n",
                "        },\n",
                "        {\n",
                "            \"name\": \"Lean_Expert_Minimal\",\n",
                "            \"components\": PromptComponents.from_dict({\n",
                "                \"role_description\": \"lean_expert\",\n",
                "                \"task_description\": \"standard\",\n",
                "                \"classification_types\": \"basic\",\n",
                "                \"function_definition\": \"basic\",\n",
                "                \"parsing_instructions\": \"sequential\",\n",
                "                \"output_format\": \"basic\"\n",
                "            })\n",
                "        },\n",
                "        {\n",
                "            \"name\": \"Process_Engineer_Detailed\",\n",
                "            \"components\": PromptComponents.from_dict({\n",
                "                \"role_description\": \"process_engineer\",\n",
                "                \"task_description\": \"efficiency_focused\",\n",
                "                \"classification_types\": \"detailed\",\n",
                "                \"function_definition\": \"detailed\",\n",
                "                \"parsing_instructions\": \"holistic\",\n",
                "                \"output_format\": \"structured\",\n",
                "                \"example_output\": \"complex_process\",\n",
                "                \"additional_guidelines\": \"context_aware\"\n",
                "            })\n",
                "        }\n",
                "    ]\n",
                "    return variations\n",
                "\n",
                "def get_models() -> Dict[str, ValueClassificationComponentsGPT]:\n",
                "    models = {}\n",
                "    variations = create_prompt_components_variations()\n",
                "    \n",
                "    for variation in variations:\n",
                "        model_name = f\"GPT-3.5-Value-Classification-{variation['name']}\"\n",
                "        model = ValueClassificationComponentsGPT(prompt_components=variation['components'])\n",
                "        models[model_name] = model\n",
                "    \n",
                "    return models\n",
                "\n",
                "def process_activity(sector: str, activity: str) -> None:\n",
                "    logging.info(f\"Processing sector: {sector}, activity: {activity}\")\n",
                "    \n",
                "    try:\n",
                "        # Load activity breakdown and ground truth\n",
                "        _, activity_breakdown_path, ground_truth_path, _ = get_file_paths(sector, activity, \"dummy\")\n",
                "        activity_breakdown = ProcessModel.from_json(activity_breakdown_path)\n",
                "        ground_truth = ProcessModel.from_json(ground_truth_path)\n",
                "        \n",
                "        activity_breakdown_dict = activity_breakdown.get_activity_breakdown_dict()\n",
                "        \n",
                "        models = get_models()\n",
                "        \n",
                "        for model_name, model in models.items():\n",
                "            logging.info(f\"Processing with model: {model_name}\")\n",
                "            \n",
                "            test_path, _, _, response_path = get_file_paths(sector, activity, model_name)\n",
                "            \n",
                "            # Ensure test directory exists\n",
                "            os.makedirs(test_path, exist_ok=True)\n",
                "            \n",
                "            # Get model response\n",
                "            response = model.value_classification_step_level(activity_breakdown_dict)\n",
                "            \n",
                "            # Process model response\n",
                "            model_activity_breakdown = ProcessModel.from_json(activity_breakdown_path)\n",
                "            model_activity_breakdown.classify_substeps(response)\n",
                "            \n",
                "            # Compare results\n",
                "            comparison_metrics = compare_value_classifications(model_activity_breakdown, ground_truth)\n",
                "            \n",
                "            response_dict = {\n",
                "                \"model\": {\n",
                "                    \"name\": model_name,\n",
                "                    \"components\": model.prompt_components[\"_raw_input\"]\n",
                "                },\n",
                "                \"response\": model_activity_breakdown.to_dict(),\n",
                "                \"metrics\": comparison_metrics.to_dict()\n",
                "            }\n",
                "            \n",
                "            # Save results\n",
                "            with open(response_path, 'w') as f:\n",
                "                json.dump(response_dict, f, indent=4)\n",
                "            \n",
                "            logging.info(f\"Successfully processed and saved results for {sector}/{activity} with {model_name}\")\n",
                "    \n",
                "    except Exception as e:\n",
                "        logging.error(f\"Error processing {sector}/{activity}: {str(e)}\")\n",
                "        \n",
                "def save_model_configurations(models: Dict[str, ValueClassificationComponentsGPT], llm_version: str):\n",
                "    \"\"\"\n",
                "    Save the configuration of each model as a JSON file.\n",
                "    \n",
                "    :param models: Dictionary of model names and their corresponding ValueClassificationComponentsGPT instances\n",
                "    :param llm_version: Version of the LLM being used (e.g., \"gpt-4\")\n",
                "    \"\"\"\n",
                "    output_dir = os.path.join(\"test\", \"models\")\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    \n",
                "    for model_name, model in models.items():\n",
                "        config = {\n",
                "            \"model_name\": model_name,\n",
                "            \"prompt_components\": model.prompt_components,\n",
                "            \"llm_version\": llm_version\n",
                "        }\n",
                "        \n",
                "        file_name = f\"{model_name}.json\"\n",
                "        file_path = os.path.join(output_dir, file_name)\n",
                "        \n",
                "        with open(file_path, 'w') as f:\n",
                "            json.dump(config, f, indent=2)\n",
                "        \n",
                "        print(f\"Saved configuration for {model_name} to {file_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = ValueClassificationComponentsGPT(PromptComponents.from_dict({\n",
                "                \"role_description\": \"process_engineer\",\n",
                "                \"task_description\": \"standard\",\n",
                "                \"classification_types\": \"detailed\",\n",
                "                \"function_definition\": \"basic\",\n",
                "                \"parsing_instructions\": \"sequential\",\n",
                "                \"output_format\": \"basic\",\n",
                "                \"example_output\": \"complex_process\"\n",
                "            }))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<src.value_classification.value_classification_components_gpt.ValueClassificationComponentsGPT at 0x227692dc760>"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def aggregate_results(model_name: str) -> Dict:\n",
                "    results_path = os.path.join(ROOT_DIR, \"test\", \"results\")\n",
                "    overall_results = {\n",
                "        \"total_substeps\": 0,\n",
                "        \"correct_classifications\": 0,\n",
                "        \"confusion_matrix\": defaultdict(lambda: defaultdict(int)),\n",
                "        \"class_metrics\": defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0}),\n",
                "        \"process_level_metrics\": {\n",
                "            \"total_processes\": 0,\n",
                "            \"accuracies\": [],\n",
                "            \"substep_counts\": [],\n",
                "            \"f1_scores\": [],\n",
                "        },\n",
                "        \"misclassification_rates\": defaultdict(int),\n",
                "    }\n",
                "\n",
                "    for sector in os.listdir(results_path):\n",
                "        sector_path = os.path.join(results_path, sector)\n",
                "        if os.path.isdir(sector_path):\n",
                "            for activity in os.listdir(sector_path):\n",
                "                activity_path = os.path.join(sector_path, activity)\n",
                "                if os.path.isdir(activity_path):\n",
                "                    response_file = os.path.join(activity_path, f\"{model_name}_response.json\")\n",
                "                    if os.path.exists(response_file):\n",
                "                        with open(response_file, 'r') as f:\n",
                "                            response_data = json.load(f)\n",
                "                        \n",
                "                        try:\n",
                "                            metrics = response_data['metrics']\n",
                "                            overall_results['total_substeps'] += metrics['total_substeps']\n",
                "                            overall_results['correct_classifications'] += metrics['correct_classifications']\n",
                "\n",
                "                            # Process-level metrics\n",
                "                            overall_results['process_level_metrics']['total_processes'] += 1\n",
                "                            overall_results['process_level_metrics']['accuracies'].append(metrics['accuracy'])\n",
                "                            overall_results['process_level_metrics']['substep_counts'].append(metrics['total_substeps'])\n",
                "\n",
                "                            process_f1_scores = []\n",
                "                            for true_class, pred_dict in metrics['confusion_matrix'].items():\n",
                "                                for pred_class, count in pred_dict.items():\n",
                "                                    overall_results['confusion_matrix'][true_class][pred_class] += count\n",
                "                                    if true_class == pred_class:\n",
                "                                        overall_results['class_metrics'][true_class]['tp'] += count\n",
                "                                    else:\n",
                "                                        overall_results['class_metrics'][true_class]['fn'] += count\n",
                "                                        overall_results['class_metrics'][pred_class]['fp'] += count\n",
                "                                        overall_results['misclassification_rates'][f\"{true_class}_as_{pred_class}\"] += count\n",
                "\n",
                "                            # Calculate process-level F1 score\n",
                "                            for class_name in overall_results['class_metrics'].keys():\n",
                "                                tp = metrics['confusion_matrix'].get(class_name, {}).get(class_name, 0)\n",
                "                                fp = sum(metrics['confusion_matrix'].get(other_class, {}).get(class_name, 0) for other_class in overall_results['class_metrics'].keys() if other_class != class_name)\n",
                "                                fn = sum(metrics['confusion_matrix'].get(class_name, {}).get(other_class, 0) for other_class in overall_results['class_metrics'].keys() if other_class != class_name)\n",
                "                                \n",
                "                                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "                                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "                                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "                                process_f1_scores.append(f1_score)\n",
                "                            \n",
                "                            overall_results['process_level_metrics']['f1_scores'].append(sum(process_f1_scores) / len(process_f1_scores) if process_f1_scores else 0)\n",
                "\n",
                "                        except Exception as e:\n",
                "                            logging.warning(f\"Failed response for {sector}/{activity} with {model_name}: {str(e)}\")\n",
                "                            continue\n",
                "\n",
                "    # Calculate overall accuracy\n",
                "    overall_results['accuracy'] = overall_results['correct_classifications'] / overall_results['total_substeps']\n",
                "\n",
                "    # Calculate precision, recall, and F1 score for each class\n",
                "    class_f1_scores = []\n",
                "    class_precisions = []\n",
                "    class_recalls = []\n",
                "    for class_name, metrics in overall_results['class_metrics'].items():\n",
                "        tp = metrics['tp']\n",
                "        fp = metrics['fp']\n",
                "        fn = metrics['fn']\n",
                "        \n",
                "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "\n",
                "        overall_results['class_metrics'][class_name].update({\n",
                "            'precision': precision,\n",
                "            'recall': recall,\n",
                "            'f1_score': f1_score\n",
                "        })\n",
                "        \n",
                "        class_f1_scores.append(f1_score)\n",
                "        class_precisions.append(precision)\n",
                "        class_recalls.append(recall)\n",
                "\n",
                "    # Calculate macro average scores\n",
                "    overall_results['macro_avg_f1_score'] = sum(class_f1_scores) / len(class_f1_scores)\n",
                "    overall_results['macro_avg_precision'] = sum(class_precisions) / len(class_precisions)\n",
                "    overall_results['macro_avg_recall'] = sum(class_recalls) / len(class_recalls)\n",
                "\n",
                "    # Calculate weighted average scores\n",
                "    total_samples = sum(metrics['tp'] + metrics['fn'] for metrics in overall_results['class_metrics'].values())\n",
                "    overall_results['weighted_avg_f1_score'] = sum(metrics['f1_score'] * (metrics['tp'] + metrics['fn']) / total_samples for metrics in overall_results['class_metrics'].values())\n",
                "    overall_results['weighted_avg_precision'] = sum(metrics['precision'] * (metrics['tp'] + metrics['fn']) / total_samples for metrics in overall_results['class_metrics'].values())\n",
                "    overall_results['weighted_avg_recall'] = sum(metrics['recall'] * (metrics['tp'] + metrics['fn']) / total_samples for metrics in overall_results['class_metrics'].values())\n",
                "\n",
                "    # Calculate process-level average metrics\n",
                "    process_metrics = overall_results['process_level_metrics']\n",
                "    process_metrics['avg_accuracy'] = sum(process_metrics['accuracies']) / process_metrics['total_processes']\n",
                "    process_metrics['avg_substeps'] = sum(process_metrics['substep_counts']) / process_metrics['total_processes']\n",
                "    process_metrics['avg_f1_score'] = sum(process_metrics['f1_scores']) / process_metrics['total_processes']\n",
                "    \n",
                "    # Calculate standard deviations\n",
                "    process_metrics['std_dev_accuracy'] = math.sqrt(sum((x - process_metrics['avg_accuracy'])**2 for x in process_metrics['accuracies']) / process_metrics['total_processes'])\n",
                "    process_metrics['std_dev_substeps'] = math.sqrt(sum((x - process_metrics['avg_substeps'])**2 for x in process_metrics['substep_counts']) / process_metrics['total_processes'])\n",
                "    process_metrics['std_dev_f1_score'] = math.sqrt(sum((x - process_metrics['avg_f1_score'])**2 for x in process_metrics['f1_scores']) / process_metrics['total_processes'])\n",
                "\n",
                "    # Calculate misclassification rates\n",
                "    total_misclassifications = sum(overall_results['misclassification_rates'].values())\n",
                "    for misclass_type, count in overall_results['misclassification_rates'].items():\n",
                "        overall_results['misclassification_rates'][misclass_type] = count / total_misclassifications if total_misclassifications > 0 else 0\n",
                "\n",
                "    # Calculate Cohen's Kappa\n",
                "    observed_accuracy = overall_results['accuracy']\n",
                "    expected_accuracy = sum((sum(overall_results['confusion_matrix'][c].values()) / overall_results['total_substeps']) * \n",
                "                            (sum(pred.get(c, 0) for pred in overall_results['confusion_matrix'].values()) / overall_results['total_substeps'])\n",
                "                            for c in overall_results['class_metrics'].keys())\n",
                "    overall_results['cohens_kappa'] = (observed_accuracy - expected_accuracy) / (1 - expected_accuracy) if expected_accuracy != 1 else 0\n",
                "\n",
                "    return overall_results\n",
                "\n",
                "def save_overall_results(model_name: str, overall_results: Dict) -> None:\n",
                "    overall_test_path = os.path.join(ROOT_DIR, \"test\", \"results\", \"overall\")\n",
                "    os.makedirs(overall_test_path, exist_ok=True)\n",
                "    \n",
                "    overall_results_path = os.path.join(overall_test_path, f\"{model_name}_overall_results.json\")\n",
                "    with open(overall_results_path, 'w') as f:\n",
                "        json.dump(overall_results, f, indent=4)\n",
                "    \n",
                "    logging.info(f\"Overall results for {model_name} saved to {overall_results_path}\")\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'get_sectors' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m         save_overall_results(model_name, overall_results)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      2\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     sectors \u001b[38;5;241m=\u001b[39m \u001b[43mget_sectors\u001b[49m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sector \u001b[38;5;129;01min\u001b[39;00m sectors:\n\u001b[0;32m      5\u001b[0m         activities \u001b[38;5;241m=\u001b[39m get_activities(sector)\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'get_sectors' is not defined"
                    ]
                }
            ],
            "source": [
                "def main():\n",
                "    start = False\n",
                "    sectors = get_sectors()\n",
                "    for sector in sectors:\n",
                "        activities = get_activities(sector)\n",
                "        for activity in activities:\n",
                "            if activity == \"advanced_b2b_sales\":\n",
                "                start = True\n",
                "                \n",
                "            if not start:\n",
                "                continue\n",
                "            \n",
                "            process_activity(sector, activity)\n",
                "    \n",
                "    # After processing all activities, aggregate and save overall results for each model\n",
                "    models = get_models()\n",
                "    llm_version = \"gpt-4\"  # or whatever version you're using\n",
                "    save_model_configurations(models, llm_version)\n",
                "    for model_name in models.keys():\n",
                "        print(f\"Aggregating results for {model_name}\")\n",
                "        overall_results = aggregate_results(model_name)\n",
                "        save_overall_results(model_name, overall_results)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nw3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
